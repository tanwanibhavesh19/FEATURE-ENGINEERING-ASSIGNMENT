{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**FEATURE ENGINEERING ASSIGNMENT**"
      ],
      "metadata": {
        "id": "yLLR12L-LbSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**ASSIGNMENT QUESTIONS**\n",
        "\n",
        "1. What is a parameter ?\n",
        "  - In feature engineering, a parameter refers to a setting or value that can be adjusted to influence how a machine learning model performs.\n",
        "  - Parameters are typically used during the training phase and are learned or estimated from the data by the algorithm.\n",
        "\n",
        "2. What is correlation ?\n",
        "  - In feature engineering, correlation refers to the statistical relationship between two or more variables.\n",
        "  - It's a measure of how much two variables change together, indicating whether they tend to increase or decrease in a similar direction (positive correlation) or in opposite directions (negative correlation).\n",
        "  - Understanding correlation helps in identifying which features are most relevant for a model, as correlated features might contain redundant information.\n",
        "\n",
        "  What does negative correlation mean ?\n",
        "    - In feature engineering, a negative correlation between two features means that as one feature increases, the other tends to decrease, and vice versa.\n",
        "    - This relationship can be measured by a correlation coefficient, which ranges from -1 to +1, with negative values indicating a negative correlation.\n",
        "\n",
        "3. Define Machine Learning. What are the main components in Machine Learning ?\n",
        "  - Machine learning (ML) is a subset of artificial intelligence (AI) that enables computers to learn from data without being explicitly programmed.\n",
        "  - It involves algorithms and models that can improve with experience and data exposure, allowing them to make predictions and decisions autonomously.\n",
        "  - The main components of machine learning include data, algorithms, models, and predictions.\n",
        "\n",
        "\n",
        "4. How does loss value help in determining whether the model is good or not ?\n",
        "  - A low loss value indicates a good model because it signifies that the model's predictions are close to the actual values, meaning it's making fewer errors.\n",
        "  - The goal in training a model is to minimize the loss function, aiming for the lowest possible value.\n",
        "\n",
        "5. What are continuous and categorical variables ?\n",
        "  - In statistics and data analysis, variables can be broadly classified as either continuous or categorical.\n",
        "  - Continuous variables can take on any value within a specified range, while categorical variables represent distinct categories or groups.\n",
        "\n",
        "6.  How do we handle categorical variables in Machine Learning? What are the common techniques ?\n",
        "  - Categorical variables, which represent categories or labels, require specific handling in machine learning due to most algorithms' preference for numerical data.\n",
        "  - Common techniques include one-hot encoding, which creates binary columns for each category; label encoding, which assigns unique integer labels; and ordinal encoding, which preserves ordinal relationships between categories.\n",
        "  - Other methods like frequency encoding, target encoding, and binary encoding are also used.\n",
        "\n",
        "7. What do you mean by training and testing a dataset ?\n",
        "  - In the context of machine learning, \"training\" and \"testing\" a dataset refer to splitting a dataset into two parts: one to train the model and another to evaluate its performance on unseen data.\n",
        "  - The training dataset is used to teach the model to recognize patterns and make predictions, while the testing dataset helps determine how well the model generalizes to new, unknown data.\n",
        "\n",
        "8. What is sklearn.preprocessing ?\n",
        "  - sklearn.preprocessing is a module in the scikit-learn library that provides a variety of functions and classes for transforming raw feature vectors into a more suitable format for machine learning algorithms.\n",
        "  - This process is crucial because many algorithms benefit from standardized or normalized data, and raw data often contains inconsistencies or is not in a format that the algorithms can directly use.\n",
        "\n",
        "9.  What is a Test set ?\n",
        "  - A test set is a portion of a dataset used to evaluate the performance of a machine learning model after it has been trained.\n",
        "  - It's separate from the training data and provides an unbiased assessment of how well the model generalizes to new, unseen data.\n",
        "\n",
        "10.  How do we split data for model fitting (training and testing) in Python ?\n",
        "  - In Python, data is typically split into training and testing sets using the train_test_split function from the sklearn.model_selection module.\n",
        "  - This function randomly divides the data into two subsets: a training set used to train the machine learning model, and a testing set used to evaluate its performance on unseen data.\n",
        "\n",
        "   How do you approach a Machine Learning problem ?\n",
        "    - A structured approach to a machine learning problem typically involves defining the problem, collecting and preparing data, choosing an appropriate model, training and evaluating it, and then deploying and monitoring the model.\n",
        "    - This process ensures a systematic and efficient approach to solving machine learning challenges.\n",
        "\n",
        "11. Why do we have to perform EDA before fitting a model to the data ?\n",
        "  - Exploratory Data Analysis (EDA) before model fitting is crucial for several reasons.\n",
        "  - It allows you to identify potential errors, outliers, and patterns in the data, which can significantly impact model performance and accuracy.\n",
        "  - Additionally, EDA helps you gain a better understanding of the data's structure and relationships between variables, enabling you to make informed decisions about feature engineering and model selection.\n",
        "\n",
        "12. What is correlation ?\n",
        "  - In feature engineering, correlation refers to the statistical relationship between two or more variables, often used to understand how well one variable predicts another or to identify redundant features.\n",
        "  - It's a crucial aspect of feature selection, helping to choose the most informative features for a machine learning model.\n",
        "\n",
        "13.  What does negative correlation mean ?\n",
        "  - In feature engineering, a negative correlation between two features means that as one feature increases, the other tends to decrease, and vice versa.\n",
        "  - This relationship can be measured using a correlation coefficient, which ranges from -1 to +1, with negative values indicating a negative correlation.\n",
        "\n",
        "14. How can you find correlation between variables in Python ?\n",
        "  - To use the corrcoef() function, you need to pass in two arrays of data, one for each variable. The function will return a correlation matrix, which is a square matrix where the diagonal elements are always 1 and the off-diagonal elements indicate the correlations between different variables.\n",
        "\n",
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "  - Correlation means two variables are related in some way—they tend to move together—but it does not imply that one causes the other.\n",
        "  - Causation means one variable directly affects the other\n",
        "  - EXAMPLE:\n",
        "  - **Observation**: Kids who watch more cartoons tend to have lower grades.\n",
        "  - **Correlation**: More cartoon watching ↔ Lower grades.\n",
        "  - **Causation**? Not necessarily. Watching cartoons may take time away from studying, but there could be other reasons too, like lack of parental supervision or poor study habits.\n",
        "\n",
        "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "  - An optimizer is an algorithm used to update a model's parameters (like weights and biases) during training to minimize the loss function, aiming to improve the model's performance. Different optimizers use various strategies to navigate the parameter space, leading to faster convergence and potentially better solutions.\n",
        "  - Types of optimizers :\n",
        "    1. Stochastic Gradient Descent (SGD):\n",
        "      - SGD updates the model's parameters based on the gradient of the loss function calculated from a randomly selected subset (mini-batch) of the training data.\n",
        "      - Example: Imagine you're trying to find the lowest point in a hilly landscape. SGD takes small, random steps in the direction of the steepest descent, hoping to eventually find the minimum.\n",
        "    2. Gradient Descent with Momentum:\n",
        "      - SGD with momentum incorporates a \"momentum\" term that adds inertia to the parameter updates, making them less sensitive to noisy gradients. This helps the optimizer \"roll over\" local minima and accelerate towards the global minimum.\n",
        "      - Example: Think of a ball rolling down a hill. It accumulates momentum as it rolls, carrying it past small bumps and pits.\n",
        "    3. Adam (Adaptive Moment Estimation):\n",
        "      - Adam combines the concepts of momentum and RMSprop (Root Mean Square Propagation), using adaptive learning rates for each parameter. It calculates the first and second moments of the gradients to make more informed updates.\n",
        "      - Example: Imagine a ball rolling down a hill, but the hill changes shape. Adam can adapt to these changes by using the previous gradients to anticipate the future path.\n",
        "    4. Adagrad (Adaptive Gradient Descent):\n",
        "      - Adagrad adapts the learning rate for each parameter individually, based on the history of its gradients. It scales the learning rate inversely proportional to the square root of the sum of squared gradients.\n",
        "      - Example: Imagine a landscape with varying terrains. Adagrad adjusts its steps based on the steepness and direction of the terrain in each area.\n",
        "    5. RMSprop (Root Mean Square Propagation):\n",
        "      - RMSprop is another adaptive learning rate optimizer that uses a decaying average of squared gradients to control the step size.\n",
        "      - Example: RMSprop is like Adagrad but with a bit more forgetting of past gradients, making it more responsive to recent changes.\n",
        "\n",
        "17. What is sklearn.linear_model ?\n",
        "  - sklearn.linear_model is a module in the scikit-learn (sklearn) library, a popular Python library for machine learning.\n",
        "  - It provides a variety of linear models for regression and classification tasks. These models assume a linear relationship between the input features and the target variable.\n",
        "\n",
        "18. What does model.fit() do? What arguments must be given ?\n",
        "  - The model.fit() function is a fundamental part of training machine learning models.\n",
        "  - It's responsible for adjusting the model's internal parameters (weights and biases) to minimize a loss function, effectively learning patterns from the provided data.\n",
        "\n",
        "19. What does model.predict() do? What arguments must be given?\n",
        "  - In most machine learning frameworks (like TensorFlow/Keras or PyTorch), the model.predict() method is used to make predictions using a trained model.\n",
        "\n",
        "20. What are continuous and categorical variables?\n",
        "  - In statistics and data analysis, variables can be broadly classified as either continuous or categorical. Continuous variables can take on any value within a specified range.\n",
        "  - while categorical variables represent distinct categories or groups.\n",
        "\n",
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "  - Feature scaling, also known as normalization or standardization, is a preprocessing technique in machine learning that transforms the values of numerical features to a common scale.\n",
        "  - This process is crucial for many machine learning algorithms, as it helps to ensure that all features contribute equally to the model's performance and improves the accuracy of algorithms sensitive to the scale of data.\n",
        "\n",
        "22. How do we perform scaling in Python?\n",
        "  - Scaling in Python typically refers to feature scaling, a preprocessing step used in machine learning to standardize or normalize the range of independent variables or features of data.\n",
        "  - This is done to ensure that all features contribute equally to the model and to improve the performance of algorithms that rely on distance calculations.\n",
        "\n",
        "23. What is sklearn.preprocessing?\n",
        "  - sklearn.preprocessing is a module in the scikit-learn library that provides a variety of functions and classes for transforming raw feature vectors into a more suitable format for machine learning algorithms.\n",
        "  - This process is crucial because many algorithms benefit from standardized or normalized data, and raw data often contains inconsistencies or is not in a format that the algorithms can directly use.\n",
        "\n",
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "  - In Python, data is typically split into training and testing sets using the train_test_split function from the sklearn.model_selection module.\n",
        "  - This function randomly divides the data into two subsets: a training set used to train the machine learning model, and a testing set used to evaluate its performance on unseen data.\n",
        "\n",
        "25. Explain data encoding?\n",
        "  - Data encoding is the process of converting information from one form to another, often for efficient transmission, storage, or analysis.\n",
        "  - It essentially involves transforming data into a specific format that can be easily processed or understood by computers or other systems."
      ],
      "metadata": {
        "id": "MDHDlR6mLjSF"
      }
    }
  ]
}